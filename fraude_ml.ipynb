{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "911eb367",
   "metadata": {},
   "source": [
    "# üí≥ Detec√ß√£o de Fraudes em Transa√ß√µes Financeiras com Machine Learning\n",
    "### Disciplina: Engenharia de Machine Learning\n",
    "\n",
    "**Equipe:**  \n",
    "- Anderson de Matos Guimar√£es  \n",
    "- Gustavo Stefano Thomazinho  \n",
    "- Leonardo Rodrigues Vianna de Medeiros Lopes  \n",
    "- Renan Ost  \n",
    "\n",
    "**Professor:** Marcelo Carboni Gomes  \n",
    "**Metodologia:** CRISP-DM  \n",
    "**Dataset:** [Credit Card Fraud Detection (Kaggle)](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac22733e",
   "metadata": {},
   "source": [
    "## 1. Entendimento do Neg√≥cio\n",
    "\n",
    "Este projeto tem como objetivo desenvolver um modelo preditivo capaz de detectar transa√ß√µes fraudulentas com cart√£o de cr√©dito, utilizando algoritmos de aprendizado de m√°quina supervisionado.\n",
    "\n",
    "Fraudes financeiras representam um desafio para institui√ß√µes banc√°rias, pois causam preju√≠zos significativos e minam a confian√ßa dos clientes. O foco deste projeto √© **maximizar a detec√ß√£o de transa√ß√µes fraudulentas** (alta taxa de recall) sem comprometer excessivamente a taxa de falsos positivos.\n",
    "\n",
    "A base de dados utilizada √© real, anonimizadas por PCA, e extremamente desbalanceada ‚Äî apenas **0,172% das transa√ß√µes s√£o fraudes**. Isso imp√µe desafios na modelagem e na avalia√ß√£o de desempenho dos modelos.\n",
    "\n",
    "A metodologia adotada ser√° o **CRISP-DM**, seguindo as etapas: entendimento do neg√≥cio, entendimento dos dados, prepara√ß√£o, modelagem, avalia√ß√£o e apresenta√ß√£o.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f4df25",
   "metadata": {},
   "source": [
    "### 1.1 Documenta√ß√£o T√©cnica do Dataset\n",
    "\n",
    "Esta se√ß√£o apresenta a documenta√ß√£o t√©cnica detalhada do dataset utilizado no projeto, conforme exigido pelo professor:\n",
    "\n",
    "#### 1) R√≥tulos (nomes das vari√°veis)\n",
    "\n",
    "- `Time`: tempo (em segundos) desde a primeira transa√ß√£o registrada.\n",
    "- `Amount`: valor da transa√ß√£o em euros (‚Ç¨).\n",
    "- `V1` a `V28`: componentes principais resultantes de uma transforma√ß√£o PCA aplicada para anonimiza√ß√£o dos dados originais. Seus significados exatos n√£o s√£o p√∫blicos, mas mant√™m relev√¢ncia estat√≠stica.\n",
    "- `Class`: vari√°vel-alvo. Valores:\n",
    "  - `0` ‚Üí transa√ß√£o leg√≠tima\n",
    "  - `1` ‚Üí transa√ß√£o fraudulenta\n",
    "\n",
    "#### 2) Tipos de dados\n",
    "\n",
    "| Coluna        | Tipo de dado |\n",
    "|---------------|--------------|\n",
    "| `Time`        | `float64`    |\n",
    "| `Amount`      | `float64`    |\n",
    "| `V1` a `V28`  | `float64`    |\n",
    "| `Class`       | `int64`      |\n",
    "\n",
    "> Obs.: O dataset n√£o cont√©m valores ausentes (NaN) nem temporais no formato `datetime` (NaT).\n",
    "\n",
    "#### 3) Quantitativos\n",
    "\n",
    "- **Total de registros (linhas):** 284.807\n",
    "- **Total de vari√°veis (colunas):** 31\n",
    "- **Total de transa√ß√µes fraudulentas (`Class = 1`):** 492  \n",
    "- **Propor√ß√£o de fraudes:** aproximadamente 0,172%\n",
    "- **Transa√ß√µes leg√≠timas (`Class = 0`):** 284.315\n",
    "\n",
    "#### 4) N√∫mero de datasets\n",
    "\n",
    "- Apenas **um dataset** est√° sendo utilizado neste projeto:\n",
    "  - `creditcard.csv`\n",
    "\n",
    "#### 5) Relacionamentos\n",
    "\n",
    "- O dataset √© **auto contido** (flat table).\n",
    "- N√£o h√° relacionamento com outras tabelas ou bases externas.\n",
    "- Cada linha representa uma transa√ß√£o financeira independente.\n",
    "\n",
    "#### 6) Formato dos dados\n",
    "\n",
    "- **Formato:** `CSV` (Comma-Separated Values)\n",
    "- **Codifica√ß√£o:** UTF-8\n",
    "- **Fonte oficial:** Kaggle  \n",
    "  [Credit Card Fraud Detection ‚Äì Kaggle: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)\n",
    "\n",
    "> Este dataset foi coletado em uma parceria entre o grupo Worldline e o Machine Learning Group da Universit√© Libre de Bruxelles (ULB), e contempla transa√ß√µes realizadas por portadores de cart√µes europeus em setembro de 2013.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1457bba",
   "metadata": {},
   "source": [
    "## üîß Prepara√ß√£o Inicial\n",
    "\n",
    "Nesta c√©lula, importamos as bibliotecas necess√°rias para a an√°lise e carregamos o dataset diretamente a partir de uma URL p√∫blica. O dataset √© armazenado em um DataFrame do pandas (`df`) para posterior an√°lise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c39149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas principais\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configura√ß√µes gr√°ficas\n",
    "sns.set(style=\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Carregando o dataset via URL\n",
    "url = 'https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# C√≥pia de seguran√ßa do dataset original (para compara√ß√µes futuras)\n",
    "df_original = df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6478e9",
   "metadata": {},
   "source": [
    "## 2. Entendimento dos Dados\n",
    "\n",
    "Nesta etapa, realizamos uma explora√ß√£o inicial da estrutura do dataset para obter uma vis√£o geral da sua composi√ß√£o e qualidade.\n",
    "\n",
    "O objetivo principal √© compreender:\n",
    "\n",
    "- O tamanho da base de dados\n",
    "- A presen√ßa de valores ausentes (NaN ou NaT)\n",
    "- Os tipos de dados de cada vari√°vel\n",
    "- As primeiras e √∫ltimas linhas da base\n",
    "- Estat√≠sticas descritivas b√°sicas\n",
    "\n",
    "Essas an√°lises s√£o essenciais para orientar as decis√µes nas etapas seguintes de prepara√ß√£o e modelagem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564e422b",
   "metadata": {},
   "source": [
    "### üîé Verificando o tamanho do dataset\n",
    "\n",
    "O m√©todo `df.shape` retorna uma tupla com o n√∫mero de linhas e colunas do DataFrame. Essa √© uma informa√ß√£o fundamental para entendermos a dimens√£o da base de dados que ser√° analisada e modelada.\n",
    "\n",
    "- O primeiro valor representa o n√∫mero de registros (transa√ß√µes).\n",
    "- O segundo valor representa o n√∫mero de vari√°veis (colunas).\n",
    "\n",
    "Essa an√°lise nos d√° uma ideia inicial da escala do problema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0be49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando o n√∫mero de linhas e colunas\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de0c950",
   "metadata": {},
   "source": [
    "### üßæ Visualizando as primeiras linhas do dataset\n",
    "\n",
    "O m√©todo `df.head()` exibe, por padr√£o, as **cinco primeiras linhas** do DataFrame. Isso permite observar:\n",
    "\n",
    "- A estrutura dos dados\n",
    "- A ordem das colunas\n",
    "- Exemplos reais de valores presentes\n",
    "- Poss√≠veis inconsist√™ncias ou padr√µes\n",
    "\n",
    "√â uma das primeiras formas de \"enxergar\" o conte√∫do da base.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5a5f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando as primeiras 5 linhas do dataset\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2df512d",
   "metadata": {},
   "source": [
    "### üìÑ Visualizando as √∫ltimas linhas do dataset\n",
    "\n",
    "O m√©todo `df.tail()` retorna, por padr√£o, as **cinco √∫ltimas linhas** do DataFrame.  \n",
    "Isso √© √∫til para verificar se h√° algum comportamento at√≠pico no final da base de dados, como:\n",
    "\n",
    "- Registros incompletos\n",
    "- Campos zerados ou nulos\n",
    "- Mudan√ßas de padr√£o\n",
    "\n",
    "Tamb√©m complementa a visualiza√ß√£o iniciada com `df.head()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350bf1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando as 5 √∫ltimas linhas do dataset\n",
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a784ead4",
   "metadata": {},
   "source": [
    "### üß± Verificando estrutura e tipos de dados com `.info()`\n",
    "\n",
    "O m√©todo `df.info()` fornece um resumo das colunas do DataFrame, incluindo:\n",
    "\n",
    "- N√∫mero total de entradas (linhas)\n",
    "- Nome de cada coluna\n",
    "- Quantidade de valores n√£o nulos\n",
    "- Tipo de dado de cada coluna\n",
    "\n",
    "√â fundamental para:\n",
    "\n",
    "- Identificar valores ausentes (NaN ou NaT)\n",
    "- Verificar a consist√™ncia dos tipos de dados (ex: `float64`, `int64`)\n",
    "- Estimar o uso de mem√≥ria do dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd2ea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo da estrutura do DataFrame\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79668b80",
   "metadata": {},
   "source": [
    "### üìä Estat√≠sticas descritivas com `.describe()`\n",
    "\n",
    "O m√©todo `df.describe()` gera estat√≠sticas descritivas para as colunas num√©ricas do DataFrame, como:\n",
    "\n",
    "- M√©dia\n",
    "- Desvio padr√£o\n",
    "- Valores m√≠nimos e m√°ximos\n",
    "- Quartis (Q1, Q2 - mediana, Q3)\n",
    "\n",
    "Esses dados ajudam a identificar:\n",
    "\n",
    "- Distribui√ß√µes assim√©tricas\n",
    "- Outliers (valores extremos)\n",
    "- Escalas distintas entre vari√°veis\n",
    "\n",
    "√â especialmente √∫til antes de aplicar transforma√ß√µes como normaliza√ß√£o ou padroniza√ß√£o.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065dfd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estat√≠sticas descritivas das vari√°veis num√©ricas\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4b4ac2",
   "metadata": {},
   "source": [
    "## 3. Pr√©-processamento de Texto (An√°lise Auxiliar)\n",
    "\n",
    "Como solicitado pelo professor, inclu√≠mos nesta etapa a aplica√ß√£o de t√©cnicas de pr√©-processamento textual.  \n",
    "Embora o dataset principal (`creditcard.csv`) n√£o contenha campos de texto, constru√≠mos um **texto explicativo do projeto** como corpus base.\n",
    "\n",
    "As etapas aplicadas foram:\n",
    "\n",
    "- Minifica√ß√£o do texto (lowercase)\n",
    "- Remo√ß√£o de pontua√ß√£o, acentua√ß√£o e d√≠gitos\n",
    "- Remo√ß√£o de stopwords\n",
    "- Stemming com `PorterStemmer`\n",
    "- Vetoriza√ß√£o com TF-IDF\n",
    "\n",
    "A seguir, mostramos cada etapa aplicada ao texto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db480646",
   "metadata": {},
   "source": [
    "### üìö Importa√ß√£o das bibliotecas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d144c195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9742b4c",
   "metadata": {},
   "source": [
    "### üßæ Texto explicativo do projeto (usado como corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ee76a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_projeto = \"\"\"Este projeto tem como objetivo desenvolver um sistema inteligente capaz de detectar fraudes em transa√ß√µes financeiras \n",
    "por meio de t√©cnicas avan√ßadas de aprendizado de m√°quina supervisionado. A solu√ß√£o proposta busca identificar padr√µes \n",
    "an√¥malos em grandes volumes de dados, utilizando etapas rigorosas de pr√©-processamento textual, vetoriza√ß√£o e \n",
    "modelagem preditiva. O desempenho dos modelos ser√° avaliado com base em m√©tricas como acur√°cia, precis√£o, recall e \n",
    "F1-score, assegurando sua robustez e aplicabilidade em contextos reais. Destinado a institui√ß√µes financeiras, o \n",
    "sistema funcionar√° como uma ferramenta preventiva, refor√ßando a seguran√ßa digital e contribuindo para a mitiga√ß√£o de \n",
    "perdas decorrentes de atividades fraudulentas.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e476cdd8",
   "metadata": {},
   "source": [
    "### üßº Pr√©-processamento inicial (min√∫sculas, pontua√ß√£o, acentos, d√≠gitos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eb8929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Min√∫sculas\n",
    "texto = texto_projeto.lower()\n",
    "\n",
    "# 2. Remo√ß√£o de pontua√ß√£o\n",
    "punctuation = string.punctuation\n",
    "trantab = str.maketrans(punctuation, len(punctuation) * ' ')\n",
    "texto = texto.translate(trantab)\n",
    "\n",
    "# 3. Remo√ß√£o de acentos\n",
    "texto = unidecode(texto)\n",
    "\n",
    "# 4. Remo√ß√£o de d√≠gitos\n",
    "texto = re.sub(r'\\d+', '', texto)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc35f462",
   "metadata": {},
   "source": [
    "### ‚ùå Remo√ß√£o de stopwords (com `try-except`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff89df9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    stopwords_list = stopwords.words('portuguese')\n",
    "except:\n",
    "    nltk.download('stopwords')\n",
    "    stopwords_list = stopwords.words('portuguese')\n",
    "\n",
    "palavras = texto.split()\n",
    "palavras_filtradas = [palavra for palavra in palavras if palavra not in stopwords_list and len(palavra) > 1]\n",
    "texto_limpo = \" \".join(palavras_filtradas)\n",
    "\n",
    "print(texto_limpo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b263b98",
   "metadata": {},
   "source": [
    "### üß™ Stemming com `PorterStemmer`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bcf19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "texto_stemmed = \" \".join([stemmer.stem(palavra) for palavra in texto_limpo.split()])\n",
    "\n",
    "print(texto_stemmed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d310fead",
   "metadata": {},
   "source": [
    "### üß† Vetoriza√ß√£o com TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e65189",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = np.array([\n",
    "    texto_stemmed,\n",
    "    \"sistema inteligente detectar padrao comportamento transacao financeira\"\n",
    "])\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_result = vectorizer.fit_transform(corpus)\n",
    "\n",
    "df_tfidf = pd.DataFrame.sparse.from_spmatrix(tfidf_result, columns=vectorizer.get_feature_names_out())\n",
    "df_tfidf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49941977",
   "metadata": {},
   "source": [
    "## 4. An√°lise Explorat√≥ria de Dados (EDA)\n",
    "\n",
    "Nesta etapa, buscamos explorar visualmente os dados para compreender a distribui√ß√£o de valores, detectar padr√µes, identificar valores discrepantes (outliers) e verificar o comportamento da vari√°vel-alvo `Class`.\n",
    "\n",
    "Al√©m disso, ser√£o aplicadas transforma√ß√µes simples para comparar o \"antes e depois\" dos dados, conforme exigido na atividade da disciplina.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888fe485",
   "metadata": {},
   "source": [
    "### üéØ Distribui√ß√£o da vari√°vel-alvo `Class`\n",
    "\n",
    "Vamos visualizar a propor√ß√£o entre transa√ß√µes leg√≠timas (`Class = 0`) e fraudulentas (`Class = 1`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a3e9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Class', data=df, palette='Set2')\n",
    "plt.title('Distribui√ß√£o das Transa√ß√µes (0 = Leg√≠tima, 1 = Fraude)')\n",
    "plt.xlabel('Classe')\n",
    "plt.ylabel('Contagem')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2977e3a",
   "metadata": {},
   "source": [
    "A maior parte das transa√ß√µes √© leg√≠tima (`Class = 0`). Apenas **0,172%** dos registros representam fraudes (`Class = 1`), o que confirma que o dataset √© altamente desbalanceado.  \n",
    "Isso influenciar√° diretamente na escolha dos algoritmos e nas m√©tricas de avalia√ß√£o utilizadas posteriormente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295a1233",
   "metadata": {},
   "source": [
    "### üí∂ An√°lise da vari√°vel `Amount`\n",
    "\n",
    "Vamos analisar a distribui√ß√£o dos valores das transa√ß√µes antes de qualquer transforma√ß√£o, utilizando:\n",
    "\n",
    "- Histograma (para ver concentra√ß√£o)\n",
    "- Boxplot (para observar outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24139771",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "sns.histplot(df['Amount'], bins=50, kde=True)\n",
    "plt.title('Distribui√ß√£o dos Valores das Transa√ß√µes (Antes da Normaliza√ß√£o)')\n",
    "plt.xlabel('Valor (‚Ç¨)')\n",
    "plt.ylabel('Frequ√™ncia')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200451ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 2))\n",
    "sns.boxplot(x=df['Amount'])\n",
    "plt.title('Boxplot do Valor das Transa√ß√µes (Antes da Normaliza√ß√£o)')\n",
    "plt.xlabel('Valor (‚Ç¨)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf75af",
   "metadata": {},
   "source": [
    "Os gr√°ficos mostram que a maioria das transa√ß√µes possui valores baixos, concentrados abaixo de ‚Ç¨100.  \n",
    "Entretanto, existem outliers (valores muito altos), o que pode distorcer an√°lises e impactar algoritmos sens√≠veis √† escala, como regress√£o log√≠stica e KNN.\n",
    "\n",
    "Por isso, ser√° necess√°rio aplicar uma transforma√ß√£o de escala nos valores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be87a817",
   "metadata": {},
   "source": [
    "### üîß Normaliza√ß√£o do valor da transa√ß√£o (`Amount`)\n",
    "\n",
    "Alguns algoritmos de Machine Learning s√£o sens√≠veis √† escala dos dados. Como a vari√°vel `Amount` apresenta uma grande varia√ß√£o e outliers, vamos aplicar a **normaliza√ß√£o com `StandardScaler`**, que transforma os dados para que tenham m√©dia 0 e desvio padr√£o 1.\n",
    "\n",
    "Dessa forma, o algoritmo ser√° menos influenciado por diferen√ßas de escala.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6d9b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instanciando o normalizador\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Aplicando ao Amount\n",
    "df['Amount_Scaled'] = scaler.fit_transform(df[['Amount']])\n",
    "\n",
    "# Verificando estat√≠sticas antes e depois\n",
    "df[['Amount', 'Amount_Scaled']].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47128a5",
   "metadata": {},
   "source": [
    "### üìä Compara√ß√£o ‚ÄúAntes e Depois‚Äù com Gr√°ficos\n",
    "\n",
    "Vamos comparar a distribui√ß√£o da vari√°vel `Amount` antes e depois da transforma√ß√£o.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4213c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(14, 4))\n",
    "\n",
    "# Boxplot original\n",
    "sns.boxplot(x=df['Amount'], ax=axs[0], color='skyblue')\n",
    "axs[0].set_title('Antes da Normaliza√ß√£o')\n",
    "axs[0].set_xlabel('Amount (‚Ç¨)')\n",
    "\n",
    "# Boxplot escalado\n",
    "sns.boxplot(x=df['Amount_Scaled'], ax=axs[1], color='lightgreen')\n",
    "axs[1].set_title('Depois da Normaliza√ß√£o')\n",
    "axs[1].set_xlabel('Amount Scaled')\n",
    "\n",
    "plt.suptitle('Boxplots Comparativos - Amount Original vs Normalizado')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8d5024",
   "metadata": {},
   "source": [
    "### üìä Histograma do valor normalizado das transa√ß√µes\n",
    "\n",
    "Para complementar a visualiza√ß√£o, vamos analisar o histograma da vari√°vel `Amount_Scaled` ap√≥s a transforma√ß√£o com `StandardScaler`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7331c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "sns.histplot(df['Amount_Scaled'], bins=50, kde=True, color='green')\n",
    "plt.title('Distribui√ß√£o do Valor das Transa√ß√µes Ap√≥s Normaliza√ß√£o')\n",
    "plt.xlabel('Amount Scaled')\n",
    "plt.ylabel('Frequ√™ncia')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed925f",
   "metadata": {},
   "source": [
    "A transforma√ß√£o com `StandardScaler` centralizou os dados de `Amount_Scaled` em torno de zero e reduziu a vari√¢ncia.  \n",
    "Isso facilita a aprendizagem dos algoritmos e mitiga a influ√™ncia de outliers extremos.  \n",
    "O valor original foi mantido em `Amount` para fins de refer√™ncia, enquanto o modelo utilizar√° `Amount_Scaled`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca1d528",
   "metadata": {},
   "source": [
    "## 5. Prepara√ß√£o para a Modelagem Supervisionada\n",
    "\n",
    "Agora que os dados passaram por an√°lise explorat√≥ria e transforma√ß√µes b√°sicas, √© importante consolidar as vari√°veis que ser√£o utilizadas na modelagem supervisionada.\n",
    "\n",
    "Nesta etapa:\n",
    "\n",
    "- Criamos uma nova c√≥pia dos dados processados chamada `df_modelagem`\n",
    "- Garantimos que o DataFrame principal (`df`) permane√ßa inalterado\n",
    "- Selecionamos os atributos relevantes\n",
    "- Removemos colunas que n√£o ser√£o utilizadas (como `Time`, por exemplo)\n",
    "\n",
    "Essa separa√ß√£o torna o processo mais seguro e facilita poss√≠veis ajustes futuros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c643a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma c√≥pia segura dos dados processados para modelagem\n",
    "df_modelagem = df.copy()\n",
    "\n",
    "# Removendo colunas que n√£o ser√£o utilizadas na modelagem\n",
    "# 'Time' n√£o √© informativa para o modelo e ser√° descartada\n",
    "# 'Amount' original tamb√©m pode ser descartado, pois usaremos a vers√£o escalada\n",
    "\n",
    "df_modelagem.drop(['Time', 'Amount'], axis=1, inplace=True)\n",
    "\n",
    "# Verificando o novo DataFrame\n",
    "df_modelagem.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e4be17",
   "metadata": {},
   "source": [
    "### üìä Divis√£o do Dataset: 60% Treino | 20% Valida√ß√£o | 20% Teste\n",
    "\n",
    "De acordo com a orienta√ß√£o do professor, o conjunto de dados ser√° dividido em tr√™s partes:\n",
    "\n",
    "- **60% ‚Üí Treinamento:** Utilizado para treinar os modelos. Ser√° balanceado com SMOTE.\n",
    "- **20% ‚Üí Valida√ß√£o:** Avaliar√° o desempenho durante os testes intermedi√°rios (sem SMOTE).\n",
    "- **20% ‚Üí Teste:** Avalia√ß√£o final do modelo. N√£o ser√° tocado at√© a fase final.\n",
    "\n",
    "Essa estrat√©gia permite avaliar os modelos com maior robustez, especialmente quando combinada com **valida√ß√£o cruzada (cross-validation)** no conjunto de treino.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bb78df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividindo 20% como conjunto de teste e 80% restante para treino + valida√ß√£o\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d31003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividindo 20% como conjunto de teste e 80% restante para treino + valida√ß√£o\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0aa13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Aplicando SMOTE apenas ao conjunto de treino\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7b3d61",
   "metadata": {},
   "source": [
    "### ‚úÖ Verifica√ß√£o do Balanceamento com SMOTE\n",
    "\n",
    "A seguir, verificamos a distribui√ß√£o da vari√°vel-alvo ap√≥s o balanceamento no conjunto de treino.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c19b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=y_train_bal, palette='Set2')\n",
    "plt.title('Distribui√ß√£o da Classe ap√≥s SMOTE (Treino)')\n",
    "plt.xlabel('Classe')\n",
    "plt.ylabel('Frequ√™ncia')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bc26b0",
   "metadata": {},
   "source": [
    "## 5.2 Modelagem com Valida√ß√£o Cruzada (Cross-Validation)\n",
    "\n",
    "A valida√ß√£o cruzada (k-fold) √© uma t√©cnica que divide o conjunto de treino em *k* subconjuntos, treinando o modelo *k* vezes com subconjuntos diferentes, o que resulta em uma avalia√ß√£o mais robusta do desempenho m√©dio.\n",
    "\n",
    "Nesta etapa, utilizaremos:\n",
    "\n",
    "- **Regress√£o Log√≠stica**\n",
    "- **Random Forest**\n",
    "\n",
    "Ambos os modelos ser√£o avaliados com **cross-validation (k=5)** usando **Recall** como principal m√©trica, pois queremos **reduzir falsos negativos** em detec√ß√£o de fraudes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7fb70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, recall_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bee0611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo de regress√£o log√≠stica\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Valida√ß√£o cruzada com 5 folds\n",
    "logreg_scores = cross_val_score(\n",
    "    logreg, X_train_bal, y_train_bal, cv=5,\n",
    "    scoring=make_scorer(recall_score)\n",
    ")\n",
    "\n",
    "# Exibindo resultados\n",
    "print(\"Recall m√©dio - Regress√£o Log√≠stica:\", logreg_scores.mean())\n",
    "print(\"Scores por fold:\", logreg_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dda7ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Valida√ß√£o cruzada com 5 folds\n",
    "rf_scores = cross_val_score(\n",
    "    rf, X_train_bal, y_train_bal, cv=5,\n",
    "    scoring=make_scorer(recall_score)\n",
    ")\n",
    "\n",
    "# Exibindo resultados\n",
    "print(\"Recall m√©dio - Random Forest:\", rf_scores.mean())\n",
    "print(\"Scores por fold:\", rf_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ba07b9",
   "metadata": {},
   "source": [
    "### üß† Interpreta√ß√£o dos Resultados (Valida√ß√£o Cruzada)\n",
    "\n",
    "A valida√ß√£o cruzada retornou o **recall m√©dio** para cada modelo, permitindo comparar a capacidade de cada algoritmo em detectar fraudes (classe positiva).\n",
    "\n",
    "- **Modelos com maior recall** s√£o prefer√≠veis nesse cen√°rio, pois minimizam os falsos negativos.\n",
    "- **Random Forest** costuma ter desempenho superior em problemas com dados tabulares e desbalanceados.\n",
    "\n",
    "A seguir, usaremos os dois modelos para prever no conjunto de **valida√ß√£o (20%)**, antes de aplicar no conjunto de teste final.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a19855d",
   "metadata": {},
   "source": [
    "## 5.3 Avalia√ß√£o dos Modelos na Base de Valida√ß√£o\n",
    "\n",
    "Ap√≥s treinar os modelos com valida√ß√£o cruzada, agora avaliamos seu desempenho no **conjunto de valida√ß√£o (20%)**, que n√£o foi balanceado com SMOTE.\n",
    "\n",
    "Utilizaremos as seguintes m√©tricas:\n",
    "\n",
    "- **Recall:** Principal m√©trica no contexto de fraudes (evita falsos negativos)\n",
    "- **Precision:** Avalia a propor√ß√£o de acertos entre as previs√µes positivas\n",
    "- **F1-Score:** Harm√¥nica entre recall e precision\n",
    "- **Confusion Matrix:** Vis√£o geral da performance\n",
    "\n",
    "Essa avalia√ß√£o nos ajuda a selecionar o melhor modelo antes de aplicar no conjunto de teste final.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41de8fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abcecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando os modelos com os dados balanceados\n",
    "logreg.fit(X_train_bal, y_train_bal)\n",
    "rf.fit(X_train_bal, y_train_bal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e0f2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previs√£o na base de valida√ß√£o\n",
    "y_pred_logreg = logreg.predict(X_val)\n",
    "\n",
    "# Relat√≥rio de classifica√ß√£o\n",
    "print(\"Relat√≥rio - Regress√£o Log√≠stica\")\n",
    "print(classification_report(y_val, y_pred_logreg, zero_division=0))\n",
    "\n",
    "# Matriz de confus√£o\n",
    "ConfusionMatrixDisplay.from_predictions(y_val, y_pred_logreg, display_labels=[\"Leg√≠tima\", \"Fraude\"])\n",
    "plt.title(\"Matriz de Confus√£o - Logistic Regression\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26765cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previs√£o na base de valida√ß√£o\n",
    "y_pred_rf = rf.predict(X_val)\n",
    "\n",
    "# Relat√≥rio de classifica√ß√£o\n",
    "print(\"Relat√≥rio - Random Forest\")\n",
    "print(classification_report(y_val, y_pred_rf, zero_division=0))\n",
    "\n",
    "# Matriz de confus√£o\n",
    "ConfusionMatrixDisplay.from_predictions(y_val, y_pred_rf, display_labels=[\"Leg√≠tima\", \"Fraude\"])\n",
    "plt.title(\"Matriz de Confus√£o - Random Forest\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209a5a1a",
   "metadata": {},
   "source": [
    "### üìà Interpreta√ß√£o da Avalia√ß√£o na Valida√ß√£o\n",
    "\n",
    "A partir dos resultados das m√©tricas, especialmente o **recall da classe 1 (fraudes)**, podemos comparar qual modelo melhor identifica as transa√ß√µes fraudulentas:\n",
    "\n",
    "- Modelos com **recall mais alto e f1-score equilibrado** s√£o mais indicados.\n",
    "- O desempenho na base de valida√ß√£o √© um bom indicativo da **capacidade de generaliza√ß√£o** antes de aplicarmos ao conjunto de teste final.\n",
    "\n",
    "A seguir, o modelo com melhor desempenho ser√° aplicado no conjunto de **teste (X_test, y_test)** para obter a m√©trica final do projeto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cf4ef3",
   "metadata": {},
   "source": [
    "## 5.4 Avalia√ß√£o Final no Conjunto de Teste\n",
    "\n",
    "Agora aplicamos os modelos treinados ao conjunto de **teste final (`X_test`, `y_test`)**, que permaneceu intocado durante todo o desenvolvimento.\n",
    "\n",
    "Esse passo permite verificar a **generaliza√ß√£o real** do modelo, utilizando as mesmas m√©tricas da etapa anterior:\n",
    "\n",
    "- Matriz de Confus√£o\n",
    "- Precision, Recall, F1-Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d7f7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previs√µes finais\n",
    "y_pred_logreg_test = logreg.predict(X_test)\n",
    "y_pred_rf_test = rf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9008e261",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Relat√≥rio Final - Regress√£o Log√≠stica (Teste)\")\n",
    "print(classification_report(y_test, y_pred_logreg_test, zero_division=0))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_logreg_test, display_labels=[\"Leg√≠tima\", \"Fraude\"])\n",
    "plt.title(\"Matriz de Confus√£o - Logistic Regression (Teste)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6782076",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Relat√≥rio Final - Random Forest (Teste)\")\n",
    "print(classification_report(y_test, y_pred_rf_test, zero_division=0))\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_rf_test, display_labels=[\"Leg√≠tima\", \"Fraude\"])\n",
    "plt.title(\"Matriz de Confus√£o - Random Forest (Teste)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea7e6f9",
   "metadata": {},
   "source": [
    "### ‚úÖ Considera√ß√µes Finais da Avalia√ß√£o\n",
    "\n",
    "A avalia√ß√£o no conjunto de teste mostra a performance real do modelo escolhido, simulando um cen√°rio de produ√ß√£o.\n",
    "\n",
    "- O **modelo com melhor recall e equil√≠brio entre precis√£o e F1-score para a classe 1 (fraude)** √© o mais indicado para uso pr√°tico.\n",
    "- Neste projeto, o modelo mais eficaz ser√° documentado na se√ß√£o de entrega final e relat√≥rios.\n",
    "\n",
    "A seguir, concluiremos o projeto com observa√ß√µes finais e organiza√ß√£o dos arquivos para apresenta√ß√£o.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff6d52e",
   "metadata": {},
   "source": [
    "## 6. Conclus√µes e Pr√≥ximos Passos\n",
    "\n",
    "### ‚úÖ Conclus√µes\n",
    "\n",
    "Neste projeto, desenvolvemos um pipeline completo de Machine Learning supervisionado com foco na **detec√ß√£o de fraudes em transa√ß√µes financeiras**. A seguir, os principais destaques do processo:\n",
    "\n",
    "- **Entendimento do neg√≥cio**: Exploramos o problema da fraude financeira e analisamos a estrutura do dataset fornecido.\n",
    "- **EDA e Pr√©-processamento**:\n",
    "  - Exploramos estat√≠sticas, padr√µes e outliers.\n",
    "  - Aplicamos normaliza√ß√£o (StandardScaler) e removemos colunas irrelevantes.\n",
    "- **Balanceamento com SMOTE**: Corrigimos a alta despropor√ß√£o de classes usando sobremostragem da minoria.\n",
    "- **Divis√£o 60/20/20**: Separa√ß√£o estruturada entre treino, valida√ß√£o e teste.\n",
    "- **Modelagem com Cross-Validation**: Avaliamos `Logistic Regression` e `Random Forest`, priorizando **recall** como m√©trica-chave.\n",
    "- **Avalia√ß√£o Final**: O modelo mais eficaz foi testado no conjunto final, simulando um ambiente de produ√ß√£o.\n",
    "\n",
    "### üß† Insights\n",
    "\n",
    "- O dataset altamente desbalanceado exige aten√ß√£o especial nas m√©tricas utilizadas. **Acur√°cia sozinha √© inadequada**.\n",
    "- O uso de **recall** e **F1-score** foi essencial para entender a capacidade dos modelos em identificar fraudes reais.\n",
    "- O **Random Forest** se destacou por seu desempenho robusto, mesmo em dados desequilibrados.\n",
    "\n",
    "---\n",
    "\n",
    "### üìé Anexos e Recursos\n",
    "\n",
    "- Dataset: [Credit Card Fraud Detection ‚Äì Kaggle](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)\n",
    "- Notebook: `fraude_ml.ipynb`\n",
    "- Documenta√ß√£o detalhada: dispon√≠vel na Wiki do projeto no GitHub\n",
    "\n",
    "---\n",
    "\n",
    "üìå **Miss√£o cumprida!** O pipeline est√° pronto para ser apresentado como solu√ß√£o pr√°tica para detec√ß√£o de fraudes com ML.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
